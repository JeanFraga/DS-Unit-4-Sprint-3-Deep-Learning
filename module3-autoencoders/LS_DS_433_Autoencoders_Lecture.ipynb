{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "> An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.[1][2] The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "*At the end of the lecture you should be to*:\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
    "\n",
    "__Problem:__ Is it possible to automatically represent an image as a fixed-sized vector even if it isn’t labeled?\n",
    "\n",
    "__Solution:__ Use an autoencoder\n",
    "\n",
    "Why do we need to represent an image as a fixed-sized vector do you ask? \n",
    "\n",
    "* __Information Retrieval__\n",
    "    - [Reverse Image Search](https://en.wikipedia.org/wiki/Reverse_image_search)\n",
    "    - [Recommendation Systems - Content Based Filtering](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering)\n",
    "* __Dimensionality Reduction__\n",
    "    - [Feature Extraction](https://www.kaggle.com/c/vsb-power-line-fault-detection/discussion/78285)\n",
    "    - [Manifold Learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)\n",
    "\n",
    "We've already seen *representation learning* when we talked about word embedding modelings during our NLP week. Today we're going to achieve a similiar goal on images using *autoencoders*. An autoencoder is a neural network that is trained to attempt to copy its input to its output. Usually they are restricted in ways that allow them to copy only approximately. The model often learns useful properties of the data, because it is forced to prioritize which aspecs of the input should be copied. The properties of autoencoders have made them an important part of modern generative modeling approaches. Consider autoencoders a special case of feed-forward networks (the kind we've been studying); backpropagation and gradient descent still work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Architecture (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The *encoder* compresses the input data and the *decoder* does the reverse to produce the uncompressed version of the data to create a reconstruction of the input as accurately as possible:\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png' width=800/>\n",
    "\n",
    "The learning process gis described simply as minimizing a loss function: \n",
    "$ L(x, g(f(x))) $\n",
    "\n",
    "- $L$ is a loss function penalizing $g(f(x))$ for being dissimiliar from $x$ (such as mean squared error)\n",
    "- $f$ is the encoder function\n",
    "- $g$ is the decoder function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along\n",
    "### Extremely Simple Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"wandb\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip wandb login 358ce2801e640a67df828839c179d15370f0f4aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/ds8/autoencoder\" target=\"_blank\">https://app.wandb.ai/ds8/autoencoder</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/ds8/autoencoder/runs/zmmh9tle\" target=\"_blank\">https://app.wandb.ai/ds8/autoencoder/runs/zmmh9tle</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x167685962b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"autoencoder\", entity=\"ds8\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=1000,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose = False,\n",
    "                callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "\n",
    "\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Expected to talk about the components of autoencoder and their purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an Autoencoder (Learn)\n",
    "<a id=\"p2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "As long as our architecture maintains an hourglass shape, we can continue to add layers and create a deeper network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/ds8/autoencoder\" target=\"_blank\">https://app.wandb.ai/ds8/autoencoder</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/ds8/autoencoder/runs/8v4gj1vi\" target=\"_blank\">https://app.wandb.ai/ds8/autoencoder/runs/8v4gj1vi</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f6a9269a3708>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 callbacks=[WandbCallback()])\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\wandb\\keras\\__init__.py\u001b[0m in \u001b[0;36mnew_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compile & fit model\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "autoencoder.compile(loss='binary_crossentropy',\n",
    "                    optimizer='nadam')\n",
    "\n",
    "wandb.init(project=\"autoencoder\", entity='ds8')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=1000,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose = False,\n",
    "                callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error resolved after 0:00:12.673644, resuming normal operation.\n"
     ]
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolutional autoencoder\n",
    "\n",
    "> Since our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n",
    "\n",
    "> Let's implement one. The encoder will consist in a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 8)           584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# Create Model \n",
    "input_img = Input(shape=(28,28,1))\n",
    "l1 = Conv2D(16, (3,3), activation='relu', padding='same')(input_img)\n",
    "l2 = MaxPooling2D((2,2), padding='same')(l1)\n",
    "l3 = Conv2D(8, (3, 3), activation='relu', padding='same')(l2)\n",
    "l4 = MaxPooling2D((2, 2), padding='same')(l3)\n",
    "l5 = Conv2D(8, (3, 3), activation='relu', padding='same')(l4)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(l5)\n",
    "\n",
    "# at this point the representation is (4, 4, 8i.e. 128-dimensional representation\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/ds8/autoencoder\" target=\"_blank\">https://app.wandb.ai/ds8/autoencoder</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/ds8/autoencoder/runs/wd4s7kkm\" target=\"_blank\">https://app.wandb.ai/ds8/autoencoder/runs/wd4s7kkm</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error resolved after 0:00:15.137001, resuming normal operation.\n",
      "wandb: Network error resolved after 0:00:16.696986, resuming normal operation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x26ceb298390>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"autoencoder\", entity=\"ds8\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=300,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose=False,\n",
    "                callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 20, not 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1c72c24373b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# display original\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1076\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1077\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1412\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1414\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\matplotlib\\axes\\_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     raise ValueError(\n\u001b[1;32m---> 59\u001b[1;33m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[0;32m     60\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[0;32m     61\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
      "\u001b[1;31mValueError\u001b[0m: num must be 1 <= num <= 20, not 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 10, not 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5479b4f9bc52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_imgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1076\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1077\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1412\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1414\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\matplotlib\\axes\\_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     raise ValueError(\n\u001b[1;32m---> 59\u001b[1;33m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[0;32m     60\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[0;32m     61\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
      "\u001b[1;31mValueError\u001b[0m: num must be 1 <= num <= 10, not 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoder.predict(x_train)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).T)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will train an autoencoder at some point in the near future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval with Autoencoders (Learn)\n",
    "<a id=\"p3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A common usecase for autoencoders is for reverse image search. Let's try to draw an image and see what's most similiar in our dataset. \n",
    "\n",
    "To accomplish this we will need to slice our autoendoer in half to extract our reduced features. :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoded_imgs = encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39389127,  1.0158104 ,  0.        ,  0.        ,  0.06517133,\n",
       "        2.450819  ,  0.        ,  5.1117034 ,  0.74338543,  2.3620906 ,\n",
       "        0.        ,  0.        ,  0.        ,  2.0215404 ,  6.1629906 ,\n",
       "        0.6670714 ,  4.66508   ,  2.5439487 , 17.914988  ,  0.        ,\n",
       "        7.9524546 ,  5.3824563 ,  1.0916216 ,  6.234546  ,  0.        ,\n",
       "        0.8884269 ,  7.485719  ,  3.44194   ,  8.927442  ,  0.        ,\n",
       "        0.23894644,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_imgs[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
    "nn.fit(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.kneighbors(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You should already be familiar with KNN and similarity queries, so the key component of this section is know what to 'slice' from your autoencoder (the encoder) to extract features from your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "    - Enocder\n",
    "    - Decoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "    - Can do in Keras Easily\n",
    "    - Can use a variety of architectures\n",
    "    - Architectures must follow hourglass shape\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
    "    - Extract just the encoder to use for various tasks\n",
    "    - AE ares good for dimensionality reduction, reverse image search, and may more things. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "__References__\n",
    "- [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "- [Deep Learning Cookbook](http://shop.oreilly.com/product/0636920097471.do)\n",
    "\n",
    "__Additional Material__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:U4-S3-deep-learning] *",
   "language": "python",
   "name": "conda-env-U4-S3-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
