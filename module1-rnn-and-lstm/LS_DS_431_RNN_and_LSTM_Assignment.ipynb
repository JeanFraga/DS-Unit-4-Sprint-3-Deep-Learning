{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ï»¿\\r\\nProject Gutenbergâ\\x80\\x99s The Complete Works of William Shakespeare, by William\\r\\nShakespeare\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever.  You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org.  If you are not located in the United States, youâ\\x80\\x99ll\\r\\nhave to check the laws of the country where you are located before using\\r\\nthis ebook.\\r\\n\\r\\n\\r\\nTitle: The Complete Works of William Shakespeare\\r\\n\\r\\nAuthor: William Shakespeare\\r\\n\\r\\nRelease Date: January 1994 [EBook #100]\\r\\nLast Updated: November 7, 2019\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nThe Complete Works of William Shakespeare\\r\\n\\r\\n\\r\\n\\r\\nby William Shakespeare\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      Contents\\r\\n\\r\\n\\r\\n\\r\\n               THE SONNETS\\r\\n\\r\\n     '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(url = 'https://www.gutenberg.org/files/100/100-0.txt')\n",
    "\n",
    "r.text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  1150962\n"
     ]
    }
   ],
   "source": [
    "giant_string = (str(r.text).split(\"Contents\\r\\n\\r\\n\\r\\n\\r\\n\", 1)[1]).split(\"FINIS\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\", 1)[0]\n",
    "\n",
    "chars = list(set(giant_string))\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "indices_char = int_char\n",
    "char_indices = char_int\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in giant_string]\n",
    "\n",
    "sequences = [] # 40 characters\n",
    "next_chars = [] # 1 character\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i: i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "\n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "X = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X[i, t, char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1150962, 40, 106)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = giant_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1150962 samples\n",
      "Epoch 1/5\n",
      "1150400/1150962 [============================>.] - ETA: 0s - loss: 1.7390\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" sorrow,\n",
      "    Remember Margaret was a pr\"\n",
      " sorrow,\n",
      "    Remember Margaret was a proud me for the more the more the money,\n",
      "    And the man be the man and here a best the mind the man be the man the man be speak,\n",
      "    And a bough and the best thou shall be the more the man but the good his speak of the more the dead,\n",
      "    And the stren with his perpose and the worse and I will be the father be part\n",
      "    And the most be that with his sure him to the bost be the bount the more the\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" sorrow,\n",
      "    Remember Margaret was a pr\"\n",
      " sorrow,\n",
      "    Remember Margaret was a present shears\n",
      "Who comes the never be think the bous which I will be the fies here a mornt in the\n",
      "                                                                                                                                                                                                                                                                                                              \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" sorrow,\n",
      "    Remember Margaret was a pr\"\n",
      " sorrow,\n",
      "    Remember Margaret was a prince?\n",
      "\n",
      " [_Elets Is down willing my grave,\n",
      "    And micking of Ubcef\n",
      "that of I wisdim, I wan thy byet yous thank that come bought be\n",
      "Entervorcanion:\n",
      "  LRWARSHOSS. Thou hant remember amenboy men think we behold;\n",
      "Not nounk mine rather out soon partimanaing weâll knows avee,\n",
      "Let me\n",
      "    Ad i knot rason yet. A beut, womans.\n",
      "Could mat diumit antimitus nopse.\n",
      "\n",
      " [_Exeut Bovely,\n",
      "So , which ill\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" sorrow,\n",
      "    Remember Margaret was a pr\"\n",
      " sorrow,\n",
      "    Remember Margaret was a prince,\n",
      "Ay, I wear they the suwpaie wisding, sir; Isword of ,\n",
      " Choose this letres. How, presentru! Gowâd visto'-, and Amander\n",
      "To strong ; he! I theirselfows, abonminer your gen,\n",
      "Done give gurop, straiss; Do his facia with Pesthhafs._\n",
      "\n",
      "COester. Go be give not that leavy, bowned hifâ toeting morrus ped meaf,\n",
      "Nonishfond nofing, gow of a loesmo,\n",
      "and for till by whats. Butpo is they foRd that\n",
      "1150962/1150962 [==============================] - 158s 137us/sample - loss: 1.7389\n",
      "Epoch 2/5\n",
      "1150848/1150962 [============================>.] - ETA: 0s - loss: 1.6643\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"s of a king; are we not high?\n",
      "    High \"\n",
      "s of a king; are we not high?\n",
      "    High the lies the stand of all the such a sight and the company.\n",
      "                                                                  E ENTESS.\n",
      "                                              T WARDINE.\n",
      "                                      A IA.\n",
      "                          The patient stand and the perfort,\n",
      "    The live the fortune the sone the forth.\n",
      "                                         AMBELLE.\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"s of a king; are we not high?\n",
      "    High \"\n",
      "s of a king; are we not high?\n",
      "    High have the grace, my lord, and all the for the fair of brow one heart\n",
      "            Shall be deared a master.\n",
      "                        The lost; from my lord, which give the worn\n",
      "            For the pries to the bust seems and whither all some may not may the such forth,\n",
      "                The demons his headst the daints the forth.\n",
      "                          Whither and the gods\n",
      "                    \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"s of a king; are we not high?\n",
      "    High \"\n",
      "s of a king; are we not high?\n",
      "    High to mlositing day will in her say.Ane Come\n",
      "    Do and how my may hard you the cornony,\n",
      "  To shate you bot may an orâs.\n",
      "  LOANTE. He wife one apposs to remembance, he to co, you doof\n",
      "As I me piage for his dress theck he into. Cousin siste .\n",
      "\n",
      "ME.\n",
      "Thy heavhes of I would speakd by him in allon\n",
      "    That ments fill never to betright us epter\n",
      "        I chalve so to usteing them sloncth, oge;\n",
      " \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"s of a king; are we not high?\n",
      "    High \"\n",
      "s of a king; are we not high?\n",
      "    High leave still, your yhe hound of Anvilianaing._]\n",
      "\n",
      "TI GLLHAOGUS.\n",
      "Patist, sir; not is you after and then my friend\n",
      "    Receive Comoom by thee for my holy pause\n",
      "Bid I must unsey worded! But honouth ansCad\n",
      "termeâs had ink hadras;\n",
      "And I give know in Aneants, if he glead ? By forish?\n",
      "\n",
      "We give the sovergcing  Ears, doth, dustobse, and his,\n",
      "Three he list speaks pover stle;\n",
      "With that down dpla s\n",
      "1150962/1150962 [==============================] - 158s 137us/sample - loss: 1.6643\n",
      "Epoch 3/5\n",
      "1150528/1150962 [============================>.] - ETA: 0s - loss: 1.6100\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ou\n",
      "      took them for.\n",
      "\n",
      "\n",
      "      SECO\"\n",
      "ou\n",
      "      took them for.\n",
      "\n",
      "\n",
      "      SECOND OF SERANE.\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                 \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ou\n",
      "      took them for.\n",
      "\n",
      "\n",
      "      SECO\"\n",
      "ou\n",
      "      took them for.\n",
      "\n",
      "\n",
      "      SECOND.\n",
      "Come to this the more to the cannoted in the smally and me, doth have that have it is she better.\n",
      "    The shower of your love might me, the landers of the ying\n",
      "    To land the common to a with the princes which she words,\n",
      "    That this find me not the two part in the day,\n",
      "    Deint the tomptied to the sparrice of the place.\n",
      "\n",
      "TITIT.\n",
      "The at when you think the world the done me, and put c\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ou\n",
      "      took them for.\n",
      "\n",
      "\n",
      "      SECO\"\n",
      "ou\n",
      "      took them for.\n",
      "\n",
      "\n",
      "      SECONDY.\n",
      "Thou shalt not be grost and flails with the worth\n",
      "    In tage so face lady? If I doâs he spe\n",
      "  Bald was itsion\n",
      "So torun in thy never dost peaces.\n",
      "\n",
      "ACT IImelb"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\U4-S3-deep-learning\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "montapy and news.\n",
      "\n",
      " [_Exeunt._]\n",
      "\n",
      "SIR ANDICBRAW.\n",
      "\n",
      "Enter BASTANR. But colike Is you a beccach my mind.  Forine.\n",
      "\n",
      "ACT ICER.\n",
      "He gentlemen topteus ewuding twing gold from a bul, who that is he paoving\n",
      "    The our shin wast we\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ou\n",
      "      took them for.\n",
      "\n",
      "\n",
      "      SECO\"\n",
      "ou\n",
      "      took them for.\n",
      "\n",
      "\n",
      "      SECOND I. Morot, might dicbisine libed what bes,\n",
      "tnate to tost me, no hefble ying peotman your\n",
      "      bitted nothing chippus the go, and midhieing\n",
      "\n",
      "On enretinryâs bastame.\n",
      "The love that unyeurs thy subwifing for him. Is adever.I The effature mae, that everlue, to weavysceat.\n",
      "  ELIDAS. Well, deep again, once\n",
      "C  This livy his lengion fome.  by that,\n",
      "What moke titte Iâll have pake kenchy.\n",
      "\n",
      "A\n",
      "1150962/1150962 [==============================] - 158s 138us/sample - loss: 1.6100\n",
      "Epoch 4/5\n",
      "1150784/1150962 [============================>.] - ETA: 0s - loss: 5.0776\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"\n",
      "laughter for a month, and a good jest f\"\n",
      "\n",
      "laughter for a month, and a good jest fotsuswe d the denf arit Doyt   Mut the mhCsaet  hofl,a:\n",
      " At                                                                                                                                                                                                                                                                                                                                                    \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"\n",
      "laughter for a month, and a good jest f\"\n",
      "\n",
      "laughter for a month, and a good jest fosTriy Fx; I\n",
      " ores if I we ga fro veurtw thy shareh thlattry tr ans gratt Q CyKoulye getcwareRhat    s GaDy thaty;oufan theud,m?\n",
      " theugsw er bandern us hAvy,s t ldotr lNus thpszt any geke vorl y do e the\n",
      "\n",
      "       thefefle t pa]g thececet see wen lack gave  ;\n",
      "lS  They t arv)hwerclus ktandgeXtiagua my the\n",
      "k 'waredycncdirLe.\n",
      " ENO A s L M I Nay the e,t faraUf,        ThantLyig yelE myde,n.]\n",
      "\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"\n",
      "laughter for a month, and a good jest f\"\n",
      "\n",
      "raughter for a month, and a good jest fu de utoualrHus SanoYuhI_iFcI_   Amladdetr nituh.\n",
      "\n",
      "\n",
      " SENt ' li\n",
      ".\n",
      "ars ih\n",
      "Fo  W decon U'iel se;ovagesnt\n",
      " Othantegpiy ;a thy ta oe rol cath sh .â\n",
      "SayY co 'ro the ; thve.Nb,  mees it ro]unnthate\n",
      "As tait  Thou jodt shhuta yvite SeFfri Lhew tavh,t  Walk, co List iuwbr who  Mali ratdwUfl w widEta  xlo foreta:emodtnT!etaL. N Rid\n",
      "th, eeaRwemf oGo, Nnesy tiAtr\n",
      "s I \n",
      "\n",
      " C yipâUl   i Nerined. \n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"\n",
      "laughter for a month, and a good jest f\"\n",
      "\n",
      "laughter for a month, and a good jest forGacwlul SI,\n",
      "and[ FiQius;\n",
      "\n",
      "it fous salraotaâm1 Rinwtouvs, etes thee,Dy,  Astimart âthlatnerOsat \n",
      "thes xune rM ite\n",
      "foret,pAg Uughho,fesen ch drtir.\n",
      "kso m onder\n",
      " o- eiomsP,dY BUnLE eh Yiy Slidasdedi cie are a\n",
      "-ld  fite. serrser Eein Hiami fe d rDted sca minri.ey. That  pypyrstjv. âl\n",
      "kef ae houxpak fU.volr, Woy fa  i _m foimy, Lo otakala.vecAnh ernButyaniefe  tur[mI    it walaâiârYa\n",
      "\n",
      "rtho\n",
      "1150962/1150962 [==============================] - 157s 137us/sample - loss: 5.0786\n",
      "Epoch 5/5\n",
      "1150912/1150962 [============================>.] - ETA: 0s - loss: 6.9086\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"to chase what flies; our cage\n",
      "We make a\"\n",
      "to chase what flies; our cage\n",
      "We make an th Rbor h              âE  B T      bor we the D  S     I tha Nouâ T    a  e  To t            T Wer o  B wit \n",
      "  yt    M    \n",
      "   W The   a t\n",
      " Gen m h         ?E     T     A t       W T  \n",
      "     hi w   E I   andTb   Th      A Iue  h thr                                                                                                                                                                   \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"to chase what flies; our cage\n",
      "We make a\"\n",
      "to chase what flies; our cage\n",
      "We make anutot h\n",
      "m bThin  hin? sW eo ai  one  wa t  ou     u aioaleE  toe hrerat \n",
      " T th    T  h  ioar Cen]  nothee te hinh gr e wdu  ouvd wb    nor   \n",
      "  an?.\n",
      "  ea r  C Ini  s ane bt \n",
      "      n  r  e I  S  a  t fEIi alt  _Esire \n",
      "O    o t  w than \n",
      " j Ch AN-     h  t a pi mgl he    Wth .cs  t he w eh hk hh   w Iar   w Fearkbtet\n",
      "    hben\n",
      "Ee e the       h thiha  I itld C    ii e   Am  n  hTC   we' aw  \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"to chase what flies; our cage\n",
      "We make a\"\n",
      "to chase what flies; our cage\n",
      "We make an . \n",
      "e,d h our sdanjdnwieG\n",
      "d s a ohl Elei ule  ye asaeeaienalla  i\n",
      " t i edrhi  Wpmno ari ao In\n",
      " . f TsGdevel\n",
      "lm tameriiwâr hi n\n",
      "tg doe nr   hintien serheeyatha Fugh  O\n",
      "o lto u.\n",
      "d. ea A  wyasdhE.\n",
      "Sh t gieon.\n",
      " io._tnis noeae gofde ok.   hs trt uin, SNn rmfetis,i  E  a tinry L  theciee shd.e  odessintcing m\n",
      "gol wor,otiunety .fICl w rneltk'bgoskronc wii m Crongeta na e\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"to chase what flies; our cage\n",
      "We make a\"\n",
      "to chase what flies; our cage\n",
      "ke make auc- tcgov trarasantypptddla hod!.Ase reitrsandbMutr? rliern wi ile au\n",
      " Itges w,Aiguy sWrm\n",
      " Cey.dhih\n",
      "r ew,blikol yffort rdoum,e .,e  ,ellt p phy a nic  paiawcihnhla,.\n",
      "setyllorIe d wr\n",
      "li,e woflei horPigden.o\n",
      " u\n",
      " ohmshimghiuie fai o a h\n",
      "nctEho.Gthyeegs,?\n",
      "y me ohno\n",
      "rlos'   no-otax u  mf\n",
      "mhyWdihanogucielutalhp:sitleoltrtonnevkr s\n",
      "iaesolnRcr w dsr gcan sarllS bilso tsrt GErLn L Alot.lirt\n",
      "1150962/1150962 [==============================] - 162s 141us/sample - loss: 6.9085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a2b3178e80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
